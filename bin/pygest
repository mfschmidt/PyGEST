#!/usr/bin/env python3

""" pygest

    A command-line interface to the PyGEST library.

   commands:

        push : maximize the r-values between expression and a comparator matrix

            $ pygest push H03511009 L --minmax max
            $ pygest push H03511009 L --data /var/alt_data --samples cortical --approach exhaustive

        push : minimize the r-values between expression and a comparator matrix

            $ pygest push H03511009 L --minmax min

        order : order probes by their contribution to the r-value between expression and a comparator matrix

            $ pygest order H03511009 L --comparator dist

    options:

        --data /data       # could use any path with properly laid out expression data
        --comparator conn  # could also use dist
        --samples all      # could also use cortical
        --minmax max       # could also use min, up, down, +, -
        --cores 0          # 0 implies to use all but one; specify how many processes to spawn
        --algo smart       # could specify one, once, 1, smrt, every, evry, exhaustive
        --shuffle          # shuffle sample well_ids to generate null distribution
        --log              # override the log file to store output
        --verbose          # use flag for more verbose output to the console


"""

import os
import sys
import logging
import argparse
import socket
import datetime
import pkg_resources
import humanize

from pygest.algorithms import algorithms

# Until this project is uploaded to pypi, we must manually put our path into the python path before import.
# Pull libs from dev source (this hacked path will go away once source is published and installed properly.)
# print("Including '{}' in python libraries for access to pygest.".format(
#     os.path.dirname(os.path.dirname(__file__))
# ))
# include_path = os.path.dirname(os.path.dirname(__file__))
# sys.path.insert(0, include_path)
import pygest as ge

# PyGEST likes to manage its own threads since it knows how it's distributing them.
if 'OPENBLAS_NUM_THREADS' not in os.environ:
    os.environ['OPENBLAS_NUM_THREADS'] = '1'


def parse_args():
    # Allow the caller to specify a donor (or other AHBGE-supported sample filter)
    parser = argparse.ArgumentParser(description="""
    PyGEST client
    
    usage:
    
        pygest command donor hemisphere [options]
    
    example:
    
        pygest pushr 1009 L --samples cortex
    
    The preceding command will repeatedly remove genes to achieve the highest possible
    correlation between expression similarity and connectivity for donor H03511009's
    left hemisphere cortical samples.
    
    Available commands:
    
        pushr    : whack-a-probe repeatedly to push Mantel correlations to their limits.
                   combine with --minmax (-m) to push up or down
        order    : One time, order the probes by their contribution to the Mantel correlation.
        dryrun   : Checks for existence of data, and reports arguments/defaults.
        overview : 
        
    """)
    parser.add_argument("command",
                        help="What can PyGEST do for you?")
    parser.add_argument("donor", default="all", nargs='?',
                        help="Which ABI donor would you like to include?")
    parser.add_argument("hemisphere", default="all", nargs='?',
                        help="Which hemisphere would you like to include?")
    parser.add_argument("-s", "--samples", dest="samples", default="all",
                        help="A subset of samples (well_ids) to include")
    parser.add_argument("-c", "--comparator", dest="comparator", default="conn",
                        help="What are we comparing expression against? 'conn' or 'dist'")
    parser.add_argument("-m", "--minmax", dest="direction", default="max",
                        help="What direction to push the correlations? 'max' or 'min'?")
    parser.add_argument("-d", "--data", dest="data", nargs='?', type=str, default='/data',
                        help="Where are the BIDS and cache directories?")
    parser.add_argument("-v", "--verbose", dest="verbose", action='store_true', default=False,
                        help="Turn on output of debug information.")
    parser.add_argument("-n", "--n_cpu", "--cores", dest="cores", default="0", type=int,
                        help="How many cores should we use? 0 means to use {n_cpus} - 1")
    parser.add_argument("--algo", dest="algo", default="smrt",
                        help="How aggressive should we be in finding max/min? 'once', 'smrt', 'evry'")
    parser.add_argument("--shuffle", dest="shuffle", action='store_true', default=False,
                        help="Shuffle sample well_ids to generate null distribution.")
    parser.add_argument("--seed", dest="seed", type=int, default=0,
                        help="Provide a seed for randomizing the expression data shuffle.")
    parser.add_argument("--log", dest="log", default='',
                        help="Provide a path to a log file to save output.")
    args = parser.parse_args()

    # Translate, interpret, and derive some of the arguments non-literally
    args.donor = ge.donor_name(args.donor)

    if args.direction.lower() in ['min', 'down', '-', ]:
        args.direction = 'min'
        args.going_up = False
        args.going_down = True
    else:
        args.direction = 'max'
        args.going_up = True
        args.going_down = False

    # Standardize to a single value to avoid mis-spellings f'ing us later
    if args.algo in algorithms:
        args.algo = algorithms[args.algo]
    else:
        args.algo = algorithms['smrt']

    if args.command == "dryrun":
        args.verbose = True

    args.beginning = datetime.datetime.now()
    return parser, args


def setup(args):
    """ Create a logger and handlers, and use them while gaining access to ge.Data.

    :param args: command-line arguments
    :return: PyGEST::ExpressionData object, logger
    """

    # Set up logging, formatted with datetimes.
    log_formatter = logging.Formatter(fmt='%(asctime)s | %(message)s', datefmt='%Y-%m-%d_%H:%M:%S')
    logger = logging.getLogger('pygest')
    logger.setLevel(1)

    # Set up the console (stdout) log handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    console_handler.setLevel(logging.DEBUG if args.verbose else logging.INFO)
    logger.addHandler(console_handler)

    # Set up the file handler, if requested
    if args.command != "dryrun":
        if args.log == '':
            file_handler = logging.FileHandler(
                set_name(args) + '.log',
                mode='a'
            )
            file_handler.setFormatter(log_formatter)
            file_handler.setLevel(logging.DEBUG)
            logger.addHandler(file_handler)
        else:
            file_handler = logging.FileHandler(args.log, mode='a+')
            file_handler.setFormatter(log_formatter)
            # file_handler.setLevel(logging.DEBUG if args.verbose else logging.INFO)
            # As a design decision, heavy logging to a file is almost always desirable, without clogging stdout
            file_handler.setLevel(logging.DEBUG)
            logger.addHandler(file_handler)

    data = ge.Data(args.data, logger)

    return data, logger


# Define some functions to wrap different commands
def set_name(args):
    """ Standardize the way filenames are generated for this particular set of data.
    :param args: command-line arguments
    """
    top_dir = os.path.join(args.data, 'derivatives')
    set_dir = '_'.join([
        '-'.join(['sub', ge.donor_name(args.donor)]),
        '-'.join(['hem', args.hemisphere]),
        '-'.join(['ctx', args.samples[:3]]),
    ])
    alg_dir = '_'.join([
        '-'.join(['tgt', args.direction]),
        '-'.join(['alg', args.algo]),
    ])
    file_name = '_'.join([
        '-'.join(['sub', ge.donor_name(args.donor)]),
        '-'.join(['cmp', args.comparator[:4]]),
    ])
    if args.donor == 'test':
        file_name = '_'.join(['dt-' + args.beginning.strftime("%Y%m%d%H%M%S"), file_name])

    # Building reports and generating data require different levels of name
    if args.command == 'overview':
        new_name = os.path.join(top_dir, set_dir)
    else:
        new_name = os.path.join(top_dir, set_dir, alg_dir, file_name)

    if args.command == 'order':
        new_name = '_'.join([new_name, 'order'])
    if args.shuffle:
        new_name = '_'.join([new_name, 'NULL{0:04d}'.format(args.seed)])

    os.makedirs(os.path.dirname(os.path.abspath(new_name)), exist_ok=True)

    return new_name


def write_sidecar(base_name, start_time):
    """ Write a json file to accompany other files using base_name

    :param base_name: The full path, sans extension, of other related files
    :param start_time: the datetime to report as the time this process began
    """

    end_time = datetime.datetime.now()

    with open(base_name + '.json', 'a+') as f:
        f.write("{\n")
        f.write("    \"host\": \"{}\",\n".format(socket.gethostname()))
        f.write("    \"command\": \"{}\",\n".format(" ".join(sys.argv[:])))
        f.write("    \"blas\": \"{} thread{}\",\n".format(
            os.environ['OPENBLAS_NUM_THREADS'],
            '' if int(os.environ['OPENBLAS_NUM_THREADS']) == 1 else 's'
        ))
        f.write("    \"pygest version\": \"{}\",\n".format(pkg_resources.require("pygest")[0].version))
        f.write("    \"log\": \"{}\",\n".format(base_name + '.log'))
        f.write("    \"data\": \"{}\",\n".format(base_name + '.tsv'))
        f.write("    \"began\": \"{}\",\n".format(start_time.strftime("%Y-%m-%d %H:%M:%S")))
        f.write("    \"completed\": \"{}\",\n".format(end_time.strftime("%Y-%m-%d %H:%M:%S")))
        f.write("    \"elapsed\": \"{}\",\n".format(end_time - start_time))
        f.write("    \"duration\": \"{}\",\n".format(humanize.naturaldelta(end_time - start_time)))
        f.write("}\n")


def push_r(exp, cmp, args, logger):
    """ Figure out the most influential genes by whacking them cumulatively.

        The pandas DataFrame object is pickled to /{data}/results/maxes/{name}.df

    :param pandas.DataFrame exp: Expression DataFrame
    :param pandas.DataFrame cmp: Comparator (typically connectivity or distance) DataFrame
    :param args: Command line arguments from shell
    :param logging.Logger logger: logger object, if desired
    """

    # Should we null the distribution first?
    if args.shuffle:
        exp = ge.algorithms.shuffled(exp, cols=True, seed=args.seed)

    # Figure out our temporary and final filenames
    base_path = set_name(args)

    # If we've already done this, don't waste the time.
    if os.path.exists(base_path + '.tsv'):
        logger.info("{} already exists in {}. no need to {}imize the same correlations again.".format(
            os.path.basename(base_path + '.tsv'), os.path.abspath(base_path), args.direction
        ))
    else:
        logger.info("Removing probes to {}imize correlation.".format(args.direction))
        gene_df = ge.algorithms.push_correlation(
            exp, cmp, algo=args.algo, ascending=args.going_up,
            progress_file=base_path + '.partial.df', cores=args.cores, logger=logger
        )
        logger.info("Saving r-{}imization over gene removal to {}.".format(
            args.direction, base_path + '.tsv'
        ))
        gene_df.sort_index(ascending=False).to_csv(
            base_path + '.tsv', sep='\t', na_rep="n/a", float_format="%0.20f"
        )
        # The complete list is now written to tsv; get rid of the temporary cached list.
        os.remove(base_path + '.partial.df',)

        write_sidecar(base_path, args.beginning)


def order(exp, cmp, args, logger):
    """ Figure out the most influential genes by whacking each only once.

        The pandas DataFrame object is pickled to /{data}/results/{name}.df

    :param pandas.DataFrame exp: Expression DataFrame
    :param pandas.DataFrame cmp: Comparator (typically connectivity or distance) DataFrame
    :param args: Command line arguments from shell
    :param logging.Logger logger: logger object, if desired
    """

    base_path = set_name(args)

    # Order probes once
    probe_order = ge.algorithms.order_probes_by_r(
        exp, cmp, ascending=args.going_up, procs=args.cores, include_full=True, logger=logger
    )
    logger.info("Saving probe order to {}.".format(base_path + '.tsv'))
    probe_order.sort_values(by=['delta'], ascending=args.going_up).to_csv(
        base_path + '.tsv', sep='\t', na_rep="n/a", float_format="%0.20f"
    )

    # And leave some notes about the process
    write_sidecar(base_path, args.beginning)


def overview(data, args, logger):
    """ Save a pdf with multiple plots describing the exp and cmp data.

    :param pygest.ExpressionData data: PyGEST ExpressionData instance, already initialized
    :param args: Command line arguments from shell
    :param logging.Logger logger: logger object, if desired
    """

    base_path = set_name(args)
    the_sample = "_".join([ge.donor_name(args.donor), args.hemisphere, args.samples])
    report_path = base_path.replace('derivatives', 'reports')
    image_path = os.path.join(report_path, 'images')
    os.makedirs(image_path, exist_ok=True)

    report_file = os.path.join(report_path, the_sample + '_overview.pdf')

    logger.info("Building an overview report for {} in {}".format(the_sample, report_path))

    report_file = ge.reporting.sample_overview(data, args, report_file, logger=logger)

    logger.info("See {} for completed report.".format(report_file))


def log_status(data, args, logger):
    """ Disclose the status of sourcedata and derivatives to the logger.

    :param pygest.ExpressionData data: PyGEST ExpressionData instance, already initialized
    :param args: Command line arguments from shell
    :param logging.Logger logger: logger object, if desired
    """

    if logger is None:
        logger = logging.getLogger('pygest')

    logger.info("SOURCES:")
    data.log_status(regarding=args.donor)
    logger.info("RESULTS:")
    ge.reporting.log_status(data, args.data, regarding=args.donor, logger=logger)


def report_context(args, logger):
    """ Get started by dumping our context.
    """
    # print("Trying to report context to logger at level {}".format(logger.level))
    # for handler in logger.handlers:
    #     print("  logger {} has handler {} at level {}".format(logger.name, handler.name, handler.level))
    logger.info("--------------------------------------------------------------------------------")
    logger.info("  Command: {}".format(" ".join(sys.argv[:])))
    logger.info("  OPENBLAS_NUM_THREADS = {}".format(os.environ['OPENBLAS_NUM_THREADS']))
    logger.info("  PyGEST is running version {}".format(pkg_resources.require("pygest")[0].version))
    logger.debug("    interpretation of arguments:")
    for k in args.__dict__:
        if args.__dict__[k] is not None:
            logger.debug("      - {} = {}".format(k, args.__dict__[k]))
    logger.info("    name string for files:")
    logger.info("    '{}'".format(set_name(args)))
    logger.info("--------------------------------------------------------------------------------")


def get_expression(data, args, logger):
    """ Gather expression data.
    """
    logger.info("Gathering expression data for {}.".format(args.donor))
    if args.donor == 'test':
        expr = data.expression(probes='test', samples='test')
    else:
        expr = data.expression(
            probes='richiardi',
            samples=data.samples(donor=args.donor, hemisphere=args.hemisphere)
        )
    logger.info("    retrieved [{}-probe X {}-sample] DataFrame.".format(
        len(expr.index), len(expr.columns)
    ))
    if args.samples[:3] == 'cor':
        len_before = len(expr.columns)
        cort_samples = [well_id for well_id in data.samples('richiardi').index if well_id in expr.columns]
        expr = expr[cort_samples]
        logger.info("    cortical-only data requested, keeping {} of the original {} samples.".format(
            len(expr.columns), len_before
        ))
    return expr


def get_comparator(data, args, sample_filter, logger):
    """ Gather comparison data
    """
    if args.comparator[:4].lower() == 'conn':
        logger.info("Gathering {} connectivity data for {}.".format('INDI', args.donor))
        # We should have a square connectivity matrix from INDI, 2551 x 2551
        comp = data.connectivity('indi', samples=sample_filter)
        logger.info("    using [{} X {}] connectivity matrix.".format(len(comp.index), len(comp.columns)))
    elif args.comparator[:4].lower() == 'dist':
        logger.info("Gathering distance data for {}.".format(args.donor))
        # We need to generate a square distance matrix from selected samples
        comp = data.distance_dataframe(sample_filter)
        logger.info("    using [{} X {}] distance matrix.".format(comp.shape[0], comp.shape[1]))
    else:
        logger.warning("Expression can be assessed vs connectivity or distance.")
        logger.warning("I don't understand '{}', and cannot continue.".format(args.comparator))
        sys.exit()
    return comp


def main():
    # Handle the many command-line arguments
    parser, arguments = parse_args()

    # For short, simple commands, just run and done
    if arguments.command == "version":
        print("pygest v{}".format(pkg_resources.require("pygest")[0].version))
        sys.exit(0)

    # Instantiate and configure pygest
    data, logger = setup(arguments)

    report_context(arguments, logger)

    if arguments.command.lower() in ['pushr', 'order', ]:
        # Pull data from pygest stores
        expression = get_expression(data, arguments, logger)
        comparator = get_comparator(data, arguments, expression.columns, logger)
    else:
        expression = None
        comparator = None

    # Execute the specified task
    if arguments.command.lower() == 'pushr':
        push_r(expression, comparator, arguments, logger)
    elif arguments.command.lower() == 'order':
        order(expression, comparator, arguments, logger)
    elif arguments.command.lower() == 'dryrun':
        logger.info("{} {}".format(arguments.data, 'exists' if os.path.isdir(arguments.data) else 'is not a directory'))
    elif arguments.command.lower() == 'overview':
        overview(data, arguments, logger)
    elif arguments.command.lower() == 'status':
        log_status(data, arguments, logger)
    else:
        print("I don't recognize the command, '{}'".format(arguments.command))

    logger.info("Done")


if __name__ == '__main__':
    main()
